<!DOCTYPE HTML>
<html lang="en" class="navy" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="utf-8">
        <title>Character Ingestion Engine - Brain AI Documentation</title>
        <meta name="description" content="Complete documentation for the Brain AI cognitive architecture system">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
        <!-- Removed theme-color meta tag for better browser compatibility -->
        <!-- <meta name="theme-color" content="#ffffff"> -->

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/custom.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "navy";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
        <div id="mdbook-help-container">
            <div id="mdbook-help-popup">
                <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
                <div>
                    <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                    <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                    <p>Press <kbd>?</kbd> to show this help</p>
                    <p>Press <kbd>Esc</kbd> to hide this help</p>
                </div>
            </div>
        </div>
        <div id="body-container">
            <!-- Work around some values being stored in localStorage wrapped in quotes -->
            <script>
                try {
                    let theme = localStorage.getItem('mdbook-theme');
                    let sidebar = localStorage.getItem('mdbook-sidebar');

                    if (theme.startsWith('"') && theme.endsWith('"')) {
                        localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                    }

                    if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                        localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                    }
                } catch (e) { }
            </script>

            <!-- Set the theme before any content is loaded, prevents flash -->
            <script>
                const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
                let theme;
                try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
                if (theme === null || theme === undefined) { theme = default_theme; }
                const html = document.documentElement;
                html.classList.remove('navy')
                html.classList.add(theme);
                html.classList.add("js");
            </script>

            <input type="checkbox" id="sidebar-toggle-anchor" class="hidden" aria-label="Toggle sidebar navigation" title="Toggle sidebar navigation">

            <!-- Hide / unhide sidebar before it is displayed -->
            <script>
                let sidebar = null;
                const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
                if (document.body.clientWidth >= 1080) {
                    try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                    sidebar = sidebar || 'visible';
                } else {
                    sidebar = 'hidden';
                }
                sidebar_toggle.checked = sidebar === 'visible';
                html.classList.remove('sidebar-visible');
                html.classList.add("sidebar-" + sidebar);
            </script>

            <nav id="sidebar" class="sidebar" aria-label="Table of contents">
                <!-- populated by js -->
                <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
                <noscript>
                    <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
                </noscript>
                <div id="sidebar-resize-handle" class="sidebar-resize-handle" role="separator" aria-label="Resize sidebar" aria-orientation="vertical" tabindex="0" aria-valuenow="250" aria-valuemin="150" aria-valuemax="500">
                    <div class="sidebar-resize-indicator"></div>
                </div>
            </nav>

            <div id="page-wrapper" class="page-wrapper">

                <div class="page">

                    <div id="search-wrapper" class="hidden">
                        <form id="searchbar-outer" class="searchbar-outer">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header" aria-label="Search documentation" title="Search the Brain AI documentation">
                        </form>
                        <div id="searchresults-outer" class="searchresults-outer hidden">
                            <div id="searchresults-header" class="searchresults-header"></div>
                            <ul id="searchresults">
                            </ul>
                        </div>
                    </div>

                    <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                    <script>
                        document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                        document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                        Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                            link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                        });
                    </script>

                    <div id="content" class="content">
                        <main>
                            <h1 id="character-ingestion-engine"><a class="header" href="#character-ingestion-engine">Character Ingestion Engine</a></h1>
<p>The Character Ingestion Engine forms the foundational layer of Brain AI’s cognitive architecture. It processes text at the most granular level - individual characters - building predictive models and establishing the basis for all higher-level understanding. This component embodies the principle that sophisticated language understanding can emerge from character-level learning.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The Character Ingestion Engine operates on the principle that language understanding should start from the most basic units and build upward. Unlike traditional NLP systems that rely on pre-tokenized words, this engine discovers language patterns organically through character-by-character processing.</p>
<pre class="mermaid">graph TD
    A[Raw Text Input] --&gt; B[Character Stream]
    B --&gt; C[Context Window]
    C --&gt; D[GRU Neural Network]
    D --&gt; E[Character Predictions]
    E --&gt; F[Confidence Scoring]
    F --&gt; G[Learning Update]
    G --&gt; H[Vocabulary Update]
    H --&gt; I[Pattern Recognition]
    
    subgraph &quot;Feedback Loop&quot;
        J[Prediction Accuracy]
        K[Error Analysis]
        L[Model Adjustment]
    end
    
    E --&gt; J
    J --&gt; K
    K --&gt; L
    L --&gt; D
</pre>
<h2 id="core-architecture"><a class="header" href="#core-architecture">Core Architecture</a></h2>
<h3 id="characterpredictor"><a class="header" href="#characterpredictor">CharacterPredictor</a></h3>
<p>The heart of the Character Ingestion Engine is the <code>CharacterPredictor</code>, which uses a GRU-based neural network to predict the next character in a sequence.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CharacterPredictor {
    model: GRUModel,
    context_size: usize,
    learning_rate: f64,
    vocabulary: Arc&lt;CharacterVocab&gt;,
    prediction_cache: LruCache&lt;String, PredictionResult&gt;,
    training_buffer: VecDeque&lt;TrainingExample&gt;,
}

impl CharacterPredictor {
    /// Predict the next character given a context
    pub fn predict_next_chars(
        &amp;mut self, 
        context: &amp;[char], 
        num_predictions: usize
    ) -&gt; Result&lt;Vec&lt;CharacterPrediction&gt;&gt; {
        // Convert context to model input
        let input_tensor = self.context_to_tensor(context)?;
        
        // Forward pass through GRU
        let output = self.model.forward(&amp;input_tensor)?;
        
        // Convert output to character probabilities
        let probabilities = self.softmax(&amp;output);
        
        // Get top predictions
        let top_predictions = self.get_top_k_predictions(&amp;probabilities, num_predictions);
        
        // Cache result for future use
        let context_key = context.iter().collect::&lt;String&gt;();
        self.prediction_cache.put(context_key, top_predictions.clone());
        
        Ok(top_predictions)
    }
    
    /// Learn from a character sequence
    pub fn learn_from_sequence(&amp;mut self, sequence: &amp;[char]) -&gt; Result&lt;LearningStats&gt; {
        let mut total_loss = 0.0;
        let mut correct_predictions = 0;
        let mut total_predictions = 0;
        
        for window in sequence.windows(self.context_size + 1) {
            let context = &amp;window[..self.context_size];
            let target = window[self.context_size];
            
            // Make prediction
            let predictions = self.predict_next_chars(context, 1)?;
            let predicted_char = predictions[0].character;
            
            // Check accuracy
            if predicted_char == target {
                correct_predictions += 1;
            }
            total_predictions += 1;
            
            // Calculate loss and update model
            let loss = self.calculate_loss(context, target)?;
            total_loss += loss;
            
            // Add to training buffer
            self.training_buffer.push_back(TrainingExample {
                context: context.to_vec(),
                target,
                timestamp: SystemTime::now(),
            });
            
            // Batch training when buffer is full
            if self.training_buffer.len() &gt;= BATCH_SIZE {
                self.train_batch()?;
            }
        }
        
        Ok(LearningStats {
            accuracy: correct_predictions as f64 / total_predictions as f64,
            average_loss: total_loss / total_predictions as f64,
            examples_processed: total_predictions,
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="charactervocab"><a class="header" href="#charactervocab">CharacterVocab</a></h3>
<p>The dynamic vocabulary system that grows and adapts as new characters are encountered.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CharacterVocab {
    char_to_id: HashMap&lt;char, CharId&gt;,
    id_to_char: HashMap&lt;CharId, char&gt;,
    frequencies: HashMap&lt;char, u64&gt;,
    special_tokens: HashMap&lt;String, CharId&gt;,
    next_id: CharId,
}

impl CharacterVocab {
    /// Create vocabulary from text, discovering characters dynamically
    pub fn from_text(text: &amp;str) -&gt; Self {
        let mut vocab = Self::new();
        
        // Add special tokens
        vocab.add_special_token("&lt;UNK&gt;", UNKNOWN_TOKEN_ID);
        vocab.add_special_token("&lt;PAD&gt;", PADDING_TOKEN_ID);
        vocab.add_special_token("&lt;START&gt;", START_TOKEN_ID);
        vocab.add_special_token("&lt;END&gt;", END_TOKEN_ID);
        
        // Process characters in order of appearance
        for ch in text.chars() {
            vocab.add_or_update_char(ch);
        }
        
        vocab
    }
    
    /// Add or update character frequency
    pub fn add_or_update_char(&amp;mut self, ch: char) -&gt; CharId {
        *self.frequencies.entry(ch).or_insert(0) += 1;
        
        if let Some(&amp;id) = self.char_to_id.get(&amp;ch) {
            id
        } else {
            let id = self.next_id;
            self.char_to_id.insert(ch, id);
            self.id_to_char.insert(id, ch);
            self.next_id += 1;
            id
        }
    }
    
    /// Get character statistics
    pub fn get_statistics(&amp;self) -&gt; VocabStats {
        VocabStats {
            total_characters: self.char_to_id.len(),
            most_frequent: self.get_most_frequent_chars(10),
            least_frequent: self.get_least_frequent_chars(10),
            coverage: self.calculate_coverage(),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="key-algorithms"><a class="header" href="#key-algorithms">Key Algorithms</a></h2>
<h3 id="1-context-aware-character-prediction"><a class="header" href="#1-context-aware-character-prediction">1. Context-Aware Character Prediction</a></h3>
<p>The engine uses a sliding context window to predict the next character based on previous characters:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn predict_with_context(
    &amp;self, 
    context: &amp;[char], 
    temperature: f64
) -&gt; Result&lt;CharacterPrediction&gt; {
    // Encode context characters to IDs
    let context_ids: Vec&lt;CharId&gt; = context
        .iter()
        .map(|&amp;ch| self.vocabulary.get_char_id(ch).unwrap_or(UNKNOWN_TOKEN_ID))
        .collect();
    
    // Create input tensor
    let input = Tensor::from_slice(&amp;context_ids, &amp;[1, context_ids.len()])?;
    
    // Forward pass
    let hidden = self.model.init_hidden(1)?;
    let (output, _) = self.model.forward(&amp;input, &amp;hidden)?;
    
    // Apply temperature scaling
    let scaled_logits = &amp;output / temperature;
    let probabilities = softmax(&amp;scaled_logits, -1)?;
    
    // Sample from distribution
    let char_id = self.sample_from_distribution(&amp;probabilities)?;
    let character = self.vocabulary.get_char(char_id)?;
    let confidence = probabilities[char_id as usize];
    
    Ok(CharacterPrediction {
        character,
        confidence,
        alternatives: self.get_alternative_predictions(&amp;probabilities, 5)?,
        context_used: context.to_vec(),
    })
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-adaptive-learning-rate"><a class="header" href="#2-adaptive-learning-rate">2. Adaptive Learning Rate</a></h3>
<p>The learning rate adapts based on prediction accuracy and loss trends:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct AdaptiveLearningRate {
    base_rate: f64,
    current_rate: f64,
    accuracy_history: VecDeque&lt;f64&gt;,
    loss_history: VecDeque&lt;f64&gt;,
    patience: usize,
    reduction_factor: f64,
    improvement_threshold: f64,
}

impl AdaptiveLearningRate {
    pub fn update_rate(&amp;mut self, accuracy: f64, loss: f64) -&gt; f64 {
        self.accuracy_history.push_back(accuracy);
        self.loss_history.push_back(loss);
        
        // Keep only recent history
        if self.accuracy_history.len() &gt; self.patience {
            self.accuracy_history.pop_front();
            self.loss_history.pop_front();
        }
        
        // Check for improvement
        if self.accuracy_history.len() &gt;= self.patience {
            let recent_avg = self.recent_average_accuracy();
            let older_avg = self.older_average_accuracy();
            
            if recent_avg &lt;= older_avg + self.improvement_threshold {
                // No significant improvement, reduce learning rate
                self.current_rate *= self.reduction_factor;
                info!("Reducing learning rate to {}", self.current_rate);
            } else if recent_avg &gt; older_avg + self.improvement_threshold * 2.0 {
                // Good improvement, slightly increase learning rate
                self.current_rate *= 1.05;
                self.current_rate = self.current_rate.min(self.base_rate);
            }
        }
        
        self.current_rate
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-pattern-recognition-and-caching"><a class="header" href="#3-pattern-recognition-and-caching">3. Pattern Recognition and Caching</a></h3>
<p>The engine recognizes recurring patterns and caches predictions for efficiency:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PatternCache {
    pattern_predictions: HashMap&lt;String, CachedPrediction&gt;,
    pattern_frequencies: HashMap&lt;String, u32&gt;,
    access_times: HashMap&lt;String, SystemTime&gt;,
    max_cache_size: usize,
}

impl PatternCache {
    pub fn get_or_predict&lt;F&gt;(
        &amp;mut self, 
        pattern: &amp;str, 
        predictor: F
    ) -&gt; Result&lt;CharacterPrediction&gt;
    where
        F: FnOnce() -&gt; Result&lt;CharacterPrediction&gt;,
    {
        // Check cache first
        if let Some(cached) = self.pattern_predictions.get(pattern) {
            // Update access time and frequency
            self.access_times.insert(pattern.to_string(), SystemTime::now());
            *self.pattern_frequencies.entry(pattern.to_string()).or_insert(0) += 1;
            
            return Ok(cached.prediction.clone());
        }
        
        // Not in cache, compute prediction
        let prediction = predictor()?;
        
        // Cache the result
        self.cache_prediction(pattern.to_string(), prediction.clone());
        
        Ok(prediction)
    }
    
    fn cache_prediction(&amp;mut self, pattern: String, prediction: CharacterPrediction) {
        // Evict old entries if cache is full
        if self.pattern_predictions.len() &gt;= self.max_cache_size {
            self.evict_least_recently_used();
        }
        
        self.pattern_predictions.insert(pattern.clone(), CachedPrediction {
            prediction,
            created_at: SystemTime::now(),
            access_count: 1,
        });
        self.access_times.insert(pattern.clone(), SystemTime::now());
        self.pattern_frequencies.insert(pattern, 1);
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration-options"><a class="header" href="#configuration-options">Configuration Options</a></h2>
<p>The Character Ingestion Engine supports extensive configuration:</p>
<pre><code class="language-toml">[components.character_ingestion]
# Model architecture
model_size = "medium"           # tiny, small, medium, large, xlarge
hidden_size = 512              # Hidden layer size
num_layers = 3                 # Number of GRU layers
dropout = 0.1                  # Dropout rate

# Training parameters
learning_rate = 0.001          # Initial learning rate
batch_size = 32                # Training batch size
sequence_length = 128          # Maximum sequence length
gradient_clip = 5.0            # Gradient clipping threshold

# Context and prediction
context_size = 64              # Context window size
num_predictions = 5            # Number of top predictions to return
temperature = 1.0              # Sampling temperature
min_confidence = 0.1           # Minimum confidence threshold

# Caching and optimization
cache_size = 10000             # Pattern cache size
enable_caching = true          # Enable pattern caching
cache_eviction_policy = "lru"  # lru, lfu, random

# Adaptive learning
adaptive_learning_rate = true  # Enable adaptive learning rate
patience = 100                 # Patience for learning rate reduction
reduction_factor = 0.8         # Learning rate reduction factor
improvement_threshold = 0.01   # Minimum improvement threshold

# Vocabulary management
max_vocab_size = 100000        # Maximum vocabulary size
min_char_frequency = 2         # Minimum frequency for vocabulary inclusion
vocab_pruning_interval = 1000  # Vocabulary pruning interval
</code></pre>
<h2 id="usage-examples"><a class="header" href="#usage-examples">Usage Examples</a></h2>
<h3 id="basic-character-prediction"><a class="header" href="#basic-character-prediction">Basic Character Prediction</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use brain::character_ingestion::{CharacterPredictor, CharacterVocab};

// Initialize the character ingestion engine
let vocab = CharacterVocab::from_text("Hello world! This is a sample text.");
let mut predictor = CharacterPredictor::new(vocab, 64)?;

// Predict next characters
let context = "Hello wor".chars().collect::&lt;Vec&lt;_&gt;&gt;();
let predictions = predictor.predict_next_chars(&amp;context, 3)?;

for prediction in predictions {
    println!("Predicted: '{}' (confidence: {:.2})", 
             prediction.character, prediction.confidence);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="training-on-text-data"><a class="header" href="#training-on-text-data">Training on Text Data</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Train the model on a text corpus
let training_text = std::fs::read_to_string("corpus.txt")?;
let characters: Vec&lt;char&gt; = training_text.chars().collect();

let learning_stats = predictor.learn_from_sequence(&amp;characters)?;
println!("Training completed:");
println!("  Accuracy: {:.2}%", learning_stats.accuracy * 100.0);
println!("  Average Loss: {:.4}", learning_stats.average_loss);
println!("  Examples: {}", learning_stats.examples_processed);
<span class="boring">}</span></code></pre></pre>
<h3 id="interactive-character-prediction"><a class="header" href="#interactive-character-prediction">Interactive Character Prediction</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::io::{self, Write};

loop {
    print!("Enter text (or 'quit' to exit): ");
    io::stdout().flush()?;
    
    let mut input = String::new();
    io::stdin().read_line(&amp;mut input)?;
    
    if input.trim() == "quit" {
        break;
    }
    
    let context: Vec&lt;char&gt; = input.trim().chars().collect();
    let predictions = predictor.predict_next_chars(&amp;context, 5)?;
    
    println!("Next character predictions:");
    for (i, pred) in predictions.iter().enumerate() {
        println!("  {}. '{}' ({:.1}%)", 
                 i + 1, pred.character, pred.confidence * 100.0);
    }
    println!();
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h2>
<h3 id="computational-complexity"><a class="header" href="#computational-complexity">Computational Complexity</a></h3>
<ul>
<li><strong>Training</strong>: O(n × m × h) where n = sequence length, m = model size, h = hidden size</li>
<li><strong>Prediction</strong>: O(m × h) for single character prediction</li>
<li><strong>Memory</strong>: O(v + c + m) where v = vocabulary size, c = cache size, m = model parameters</li>
</ul>
<h3 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Model Size</th><th>Training Speed</th><th>Prediction Speed</th><th>Memory Usage</th><th>Accuracy</th></tr></thead><tbody>
<tr><td>Tiny</td><td>1000 chars/s</td><td>50,000 preds/s</td><td>50 MB</td><td>85%</td></tr>
<tr><td>Small</td><td>800 chars/s</td><td>30,000 preds/s</td><td>150 MB</td><td>90%</td></tr>
<tr><td>Medium</td><td>500 chars/s</td><td>15,000 preds/s</td><td>400 MB</td><td>93%</td></tr>
<tr><td>Large</td><td>200 chars/s</td><td>8,000 preds/s</td><td>1.2 GB</td><td>95%</td></tr>
<tr><td>XLarge</td><td>100 chars/s</td><td>4,000 preds/s</td><td>3.5 GB</td><td>97%</td></tr>
</tbody></table>
</div>
<h3 id="optimization-tips"><a class="header" href="#optimization-tips">Optimization Tips</a></h3>
<ol>
<li><strong>Batch Processing</strong>: Process multiple sequences in batches for better GPU utilization</li>
<li><strong>Caching</strong>: Enable pattern caching for repetitive text patterns</li>
<li><strong>Model Size</strong>: Choose appropriate model size based on accuracy vs. speed requirements</li>
<li><strong>Context Size</strong>: Larger context improves accuracy but increases computation</li>
<li><strong>Vocabulary Pruning</strong>: Regular vocabulary pruning keeps memory usage manageable</li>
</ol>
<h2 id="integration-patterns"><a class="header" href="#integration-patterns">Integration Patterns</a></h2>
<h3 id="with-segment-discovery"><a class="header" href="#with-segment-discovery">With Segment Discovery</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Character predictions feed into segment discovery
let predictions = character_predictor.predict_next_chars(&amp;context, 10)?;
let segment_boundaries = segment_discovery.analyze_predictions(&amp;predictions)?;

// Feedback loop: segment boundaries improve character prediction
character_predictor.update_with_segment_feedback(&amp;segment_boundaries)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="with-memory-system"><a class="header" href="#with-memory-system">With Memory System</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Store character patterns in memory for future reference
let pattern_memory = MemoryItem {
    content: format!("Pattern: {} -&gt; {}", context_str, predicted_char),
    importance: prediction.confidence,
    memory_type: MemoryType::Pattern,
    timestamp: SystemTime::now(),
};

memory_system.store_working_memory(pattern_memory)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="with-performance-monitoring"><a class="header" href="#with-performance-monitoring">With Performance Monitoring</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Monitor character ingestion performance
let metrics = CharacterIngestionMetrics {
    predictions_per_second: predictor.get_prediction_rate(),
    average_confidence: predictor.get_average_confidence(),
    cache_hit_rate: predictor.get_cache_hit_rate(),
    model_accuracy: predictor.get_recent_accuracy(),
    memory_usage: predictor.get_memory_usage(),
};

performance_monitor.record_component_metrics(
    ComponentId::CharacterIngestion, 
    metrics
)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h3>
<h4 id="low-prediction-accuracy"><a class="header" href="#low-prediction-accuracy">Low Prediction Accuracy</a></h4>
<p><strong>Symptoms</strong>: Character predictions are frequently wrong
<strong>Causes</strong>:</p>
<ul>
<li>Insufficient training data</li>
<li>Context window too small</li>
<li>Learning rate too high or too low</li>
<li>Model size inappropriate for data complexity</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase context size
predictor.set_context_size(128)?;

// Adjust learning rate
predictor.set_learning_rate(0.0005)?;

// Use adaptive learning rate
predictor.enable_adaptive_learning_rate()?;

// Add more training data
predictor.train_on_additional_corpus(&amp;additional_text)?;
<span class="boring">}</span></code></pre></pre>
<h4 id="memory-usage-too-high"><a class="header" href="#memory-usage-too-high">Memory Usage Too High</a></h4>
<p><strong>Symptoms</strong>: High memory consumption, potential OOM errors
<strong>Causes</strong>:</p>
<ul>
<li>Large vocabulary size</li>
<li>Large model size</li>
<li>Large cache size</li>
<li>Memory leaks in training buffer</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Prune vocabulary
predictor.prune_vocabulary(min_frequency: 5)?;

// Reduce cache size
predictor.set_cache_size(5000)?;

// Use smaller model
predictor.switch_to_model_size(ModelSize::Small)?;

// Clear training buffer periodically
predictor.clear_training_buffer()?;
<span class="boring">}</span></code></pre></pre>
<h4 id="slow-prediction-speed"><a class="header" href="#slow-prediction-speed">Slow Prediction Speed</a></h4>
<p><strong>Symptoms</strong>: Character predictions take too long
<strong>Causes</strong>:</p>
<ul>
<li>Large model size</li>
<li>Large context window</li>
<li>Cache misses</li>
<li>CPU/GPU bottlenecks</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Enable GPU acceleration
predictor.enable_gpu_acceleration()?;

// Optimize batch size
predictor.set_batch_size(64)?;

// Use prediction caching
predictor.enable_pattern_caching()?;

// Reduce context size for speed
predictor.set_context_size(32)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="debugging-tools"><a class="header" href="#debugging-tools">Debugging Tools</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Enable debug logging
predictor.set_debug_mode(true)?;

// Get detailed performance metrics
let debug_info = predictor.get_debug_info()?;
println!("Debug Info: {:#?}", debug_info);

// Analyze prediction patterns
let pattern_analysis = predictor.analyze_prediction_patterns()?;
for pattern in pattern_analysis.common_patterns {
    println!("Pattern: {} (frequency: {})", pattern.text, pattern.frequency);
}

// Visualize model internals
predictor.export_model_visualization("model_viz.html")?;
<span class="boring">}</span></code></pre></pre>
<h2 id="advanced-features"><a class="header" href="#advanced-features">Advanced Features</a></h2>
<h3 id="custom-loss-functions"><a class="header" href="#custom-loss-functions">Custom Loss Functions</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Implement custom loss function for specific use cases
pub struct FocalLoss {
    alpha: f64,
    gamma: f64,
}

impl LossFunction for FocalLoss {
    fn calculate_loss(&amp;self, predictions: &amp;Tensor, targets: &amp;Tensor) -&gt; Result&lt;Tensor&gt; {
        let ce_loss = cross_entropy_loss(predictions, targets)?;
        let pt = (-ce_loss).exp();
        let focal_weight = self.alpha * (1.0 - pt).powf(self.gamma);
        Ok(focal_weight * ce_loss)
    }
}

// Use custom loss function
predictor.set_loss_function(Box::new(FocalLoss { alpha: 0.25, gamma: 2.0 }))?;
<span class="boring">}</span></code></pre></pre>
<h3 id="multi-language-support"><a class="header" href="#multi-language-support">Multi-Language Support</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Configure for multiple languages
let mut multilang_predictor = CharacterPredictor::new_multilingual()?;
multilang_predictor.add_language("en", english_vocab)?;
multilang_predictor.add_language("es", spanish_vocab)?;
multilang_predictor.add_language("fr", french_vocab)?;

// Predict with language detection
let (prediction, detected_language) = multilang_predictor
    .predict_with_language_detection(&amp;context)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="transfer-learning"><a class="header" href="#transfer-learning">Transfer Learning</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Load pre-trained model and fine-tune
let pretrained_model = CharacterPredictor::load_pretrained("gpt-char-base")?;
let mut fine_tuned = pretrained_model.clone();

// Fine-tune on domain-specific data
fine_tuned.fine_tune(&amp;domain_specific_text, epochs: 10)?;

// Compare performance
let base_accuracy = pretrained_model.evaluate(&amp;test_data)?;
let fine_tuned_accuracy = fine_tuned.evaluate(&amp;test_data)?;
println!("Improvement: {:.2}%", 
         (fine_tuned_accuracy - base_accuracy) * 100.0);
<span class="boring">}</span></code></pre></pre>
<p>The Character Ingestion Engine provides the foundational layer for Brain AI’s cognitive architecture, enabling sophisticated language understanding to emerge from character-level learning. Its adaptive algorithms, comprehensive configuration options, and robust performance make it suitable for a wide range of applications from research to production deployment.</p>

                        </main>

                        <nav class="nav-wrapper" aria-label="Page navigation">
                            <!-- Mobile navigation buttons -->
                                <a rel="prev" href="../architecture/component-interactions.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                    <i class="fa fa-angle-left"></i>
                                </a>

                                <a rel="next prefetch" href="../components/segment-discovery.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                    <i class="fa fa-angle-right"></i>
                                </a>

                            <!-- Clear both without inline styles -->
                            <div class="clear-both"></div>
                        </nav>
                    </div>
                </div>

                <nav class="nav-wide-wrapper" aria-label="Page navigation">
                        <a rel="prev" href="../architecture/component-interactions.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                            <i class="fa fa-angle-left"></i>
                        </a>

                        <a rel="next prefetch" href="../components/segment-discovery.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                            <i class="fa fa-angle-right"></i>
                        </a>
                </nav>

            </div>



            <script>
                window.playground_line_numbers = true;
            </script>

            <script>
                window.playground_copyable = true;
            </script>

            <script src="../ace.js"></script>
            <script src="../editor.js"></script>
            <script src="../mode-rust.js"></script>
            <script src="../theme-dawn.js"></script>
            <script src="../theme-tomorrow_night.js"></script>

            <script src="../elasticlunr.min.js"></script>
            <script src="../mark.min.js"></script>
            <script src="../searcher.js"></script>

            <script src="../clipboard.min.js"></script>
            <script src="../highlight.js"></script>
            <script src="../book.js"></script>

            <!-- Custom JS scripts -->
            <script src="../theme/custom.js"></script>

        </div>
    </body>
</html> 